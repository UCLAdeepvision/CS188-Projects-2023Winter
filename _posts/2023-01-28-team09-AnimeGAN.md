---
layout: post
comments: true
title: Anime ViT-StyleGAN2 
author: Yuxi Chang
date: 2023-2-26
---


> Generative adversarial network (GAN) is a type of generative nueral network capable of varies tasks such as image creation, super-resolution, and image classifications[1]. This project will explore the usage of GAN model on the specific domain of **anime character** and try to find improvemnts on current GAN model by using more advanced discriminators.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
![WaifuLab]({{'/assets/images/team09/proposal_waifu_lab.png' | relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 1. Sample Output from WaifuLab (GAN)[2].

Recent progress on anime art generation has sparked a sense of excitement in both the AI and anime community. Many AI generated arts have emerged on the internet. As shown in Fig 1, it is an anime character generated using the online tool [WaifuLab](https://waifulabs.com/generate)[2] which relies on a generator model called GAN. GAN (generative adversarial network) is the base model for many current art generator AIs. The project will explore the usage of GAN on anime character generations and try different variations the discriminator to see the effect of the quality of generated anime styled images. Since the time of this project is limited, the base model used for this project is StyleGan2-ADA, which is the state of art GAN model developed by Karras T. et al and is suitable for using smaller dataset and transfer learning. 

## A Brief Review of the StyleGAN Architecture
### GAN Architecture
GAN is short for generative adversarial network. There are mainly two components in this network: A generator and a descriminator. The generator is responsable for generating "fake" image of the desired type of images, while the descriminator is responsable for telling if the generated images from the generator is the real images that belong to the desired type of image or not. Therefore, in training, the two components are opposing to each other, thus, the model is able to achieve self-supervised, data driven learning.[1]
### Style Based Generator
![Style-based generator]({{'/assets/images/team09/style-based-generator.png' | relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 2. Comparison Between Tranditional Generator and Style-based Generator[3]

StyleGAN model mainly improves upon the generator components. As illustrated in Figure 2, instead of generating image directly from latent code, the latent code $$z$$ is first transformed by the mapping layer consists of fully connected layers into new latent code $$w$$, then through affine matrix transformation $$A$$, $$w$$ is converted into "style": $$y=(y_s, y_b)$$. $$y$$ is then mixed into the synthesis network via $$AdaIN$$ operation:
$$AdaIN={y_{s,i}}\frac{x_i-\mu(x_i)}{\sigma}+y_{b,i}$$.[3]
From the equation, the feature map $x$ is first normalized, then the corresponding scale and bias is applied from the style. From the archetecture, each genearted image is based on a collection of style drew from the image samples.[3] This property of the StyleGan is particularly useful for anime character generation, as anime has unique art style and forms a distinct domain. As also described in the paper, "each styles can be expected to affect only certain aspect of the image", the architecture is capable of generating desired styled anime character as well. 

![Style-based generator revised]({{'/assets/images/team09/style-based-generator_revised.png' | relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 3. StyleGAN2 Synthesis Network[4]: StyleGAN2 shares those same properties, but further improves the synthesis network and eliminates the droplet arifacts[4].

### Adaptive Discriminator Augmentation
The ADA in the name stands for adaptive discriminator augmentation. This is useful for training StyleGAN on small data sets to prevent overfit. Data augmentation is performed on images generated by the generator before feeding into discriminator. The augmentation strength is controlled by the degree of overfitting, hence the method is adaptive.[5] This is useful for the purpose of this project, as collecting large anime pictures and training on large dataset is not feasable because of time constrains and hardware limitations. 

## Further Improvement
### Alternative Discriminators
From the previous section, the StyleGAN2-ADA already has nice properties suitable for training anime style image generative model. The improvement mainly focused on the generater, but it is also important to explore alternative discriminators. The baseline StyleADA2 uses ResNet as the backbone architecture for its discriminator. ResNet is known to be the state of the art CNN models suitable for the classification task[8] here(which is the binary classifcation task of discriminating if the input images are fake or real). Recently, the self-attention model ViT(vision transformer) emerges and shown that it can be better than the Resnet architecture in various image classification tasks[6].

![Attention Mechanism]({{'/assets/images/team09/attention.png' | relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 4. Attention Mechanism[7]

Through self attention mechnism, ViT achieve a global view of the image even in the few earlier layers instead of relying on local features in ResNet(due to convolution operations). This might be helpful in the animate domain, as the intrinsic consistancy of art style and body orientation from the distanced parts of the images is also important for a descent anime style arts. 

### Alternative Augmentation Method 
(If there are more time and budget)
Since the data domain is anime, there might be augmentation techniques that are more suitable for anime styled images. (more if it is actually done...) 
### Alternative Generative Layers
(If there are more time and budget)
Instead of using convolution layers as the building block, replace by ViT layer. It has shown by Han Z., et al., their proposed SAGAN model which utilize the self-attention machinism achieve better results compare to tranditional GAN models.[9] 
## Experiments and Results
### Dataset
![Sample Images From the Dataset]({{'/assets/images/team09/sample_real_images.png' | relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 5. Sample Images from the Dataset
All experiment will use a relatively small [dataset](https://www.kaggle.com/datasets/tianbaiyutoby/animegirl-faces) obtained from Kaggle consists of 2434 $$256\times256$$ anime faces of different styles. The dataset is preprocessed by using the `dataset_tool.py` utilities provided in the StyleGAN2-ADA code [repository](https://github.com/NVlabs/stylegan2-ada-pytorch).    
### Training Environment
All model will be trained on a single Tesla T4 GPU, using docker environment provided in the StyleGAN2-ADA code [repository](https://github.com/NVlabs/stylegan2-ada-pytorch).  
### Baseline Results
![Sample Baseline Output Images]({{'/assets/images/team09/sample_baseline_fake_images.png' | relative_url}}){: style="width: 400px; max-width: 100%;"}
*Fig 6. Sample Output Images From the Baseline Model

The above results are obtained by doing transfer learning using the original StyleGAN2 model and the model is trained for ~800K iterations. The base model used is the pretrained [ffhq-256](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl) model from NVIDIA which generate realistic human faces(a completely different domain from anime). 
### ViT Discriminator Results
As the model archtecture is changes, need to train partially from seperated pretrained ViT and pretrained Generator.\
(In-progress)
### Evaluation and Analysis
Select images generated from the same seed and compare manually (aethetic appeals etc.) as well as using FID scores etc.\
(In-progress)
## Conclusion
(In-progress)

---

## Reference

[1] Creswell A., White T., et al. "Generative Adversarial Networks: An Overview" in IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 53-65, Jan. 2018, doi: 10.1109/MSP.2017.2765202.

[2] Liu, R. "Welcome to Waifu Labs v2: How do AIs Create?", Jan, 2022. Retrieved from: https://waifulabs.com/blog/ai-creativity  

[3] Karras T., Laine S. & Alia T. ["A Style-Based Generator Architecture for Generative Adversarial Networks"](https://arxiv.org/abs/1812.04948)

[4] Karras T., et al. ["Analyzing and Improving the Image Quality of StyleGAN"](https://arxiv.org/pdf/1912.04958.pdf)

[5] Karras T., Aittala M., et al. ["Training Generative Adversarial Networks with Limited Data"](https://arxiv.org/abs/2006.06676)

[6] Dosovistskiy A., et al. ["AN IMAGE IS WORTH 16X16 WORDS:
TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"](https://arxiv.org/pdf/2010.11929.pdf)

[7] Vaswani A., et al. ["Attention is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) 

[8] Kaiming H., et al. ["Deep Residual Learning for Image Recognition"](https://arxiv.org/pdf/1512.03385.pdf)

[9] Han Z. et al. ["Self-Attention Generative Adversarial Networks"](https://arxiv.org/pdf/1805.08318.pdf)

---
