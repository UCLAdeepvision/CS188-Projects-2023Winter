---
layout: post
comments: true
title: Investigation on the limitations of CLIP
author: Mengran (Diana) Dai, Yupei Hu
date: 2022-01-29
---

> Topic: CLIP: Text-image joint embedding

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Abstract
> **Our project topic is CLIP: Text-image joint embedding. CLIP is a neural network created by open.ai that can effectively learn visual concepts from natural language supervision.Since deep learning needs a lot of data, same as vision models, we try to use the photos that are already on the internet, reducing the cost of our training. Also, CLIP can be used for classification tasks for “out of the box” tasks by simply telling the model the visual concepts of the task. However, CLIP still has poor generalization to images that are not included in the pre-training dataset; therefore, we will investigate ways to improve the generalization in the model and narrow down the limitations.**

  
 
## Reference
  
Learning Transferable Visual Models From Natural Language Supervision

[http://proceedings.mlr.press/v139/radford21a/radford21a.pdf](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)

 
Language Models are Few-Shot Learners

[https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)


Self-training with Noisy Student improves ImageNet classification

[https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf)
[1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2016.

  



<a href="https://colab.research.google.com/github/DianaDai426/CS188-Projects-2023Winter/blob/main/CLIP.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
# `Team-19 clip`
### Midpoint Progress

    We plan to use a pretrained clip model and run a few experiments of face recognition to test its robustness on a image dataset of NBA players. 
    To better calculate the accuracy of the model, we plan to use the 95 percentile of the distance between the query embedding and the query result embedding as a threshold of valid query.
    We plan to conduct data poisoning attacks on a NBA player dataset using Fawkes and LowKey tools, and use the unperturbed, Fawkes cloaked, and LowKey attacked versions of the images as query images to search clip image dataset.
    We plan to conduct this experiment on several CLIP models to compare their accuracy and get insight on the model robustness. 

Experiment design adapted from
https://arxiv.org/pdf/2301.07315.pdf

Fawkes tool:
https://github.com/Shawn-Shan/fawkes

Lowkey tool:
https://github.com/ftramer/FaceCure

CLIP model:
https://github.com/rom1504/clip-retrieval 
```
## Install
%pip install clip-retrieval img2dataset
!pip install -r ./clip-requirements.txt
## Setup
from IPython.display import Image, display
from clip_retrieval.clip_client import ClipClient, Modality

IMAGE_BASE_URL = "https://github.com/rom1504/clip-retrieval/raw/main/tests/test_clip_inference/test_images/"

def log_result(result):
    id, caption, url, similarity = result["id"], result["caption"], result["url"], result["similarity"]
    print(f"id: {id}")
    print(f"caption: {caption}")
    print(f"url: {url}")
    print(f"similarity: {similarity}")
    display(Image(url=url, unconfined=True))

client = ClipClient(
    url="https://knn.laion.ai/knn-service",
    indice_name="laion5B-L-14",
    aesthetic_score=9,
    aesthetic_weight=0.5,
    modality=Modality.IMAGE,
    num_images=10,
)

## Query by text
cat_results = client.query(text="an image of a cat")
cat_results[0]
## Query by image
beach_results = client.query(image="https://github.com/rom1504/clip-retrieval/raw/main/tests/test_clip_inference/test_images/321_421.jpg")
log_result(beach_results[0])
## Query by embedding
import clip  # pylint: disable=import-outside-toplevel
import torch

model, preprocess = clip.load("ViT-L/14", device="cpu", jit=True)
# import clip  # pylint: disable=import-outside-toplevel
# import torch

# model, preprocess = clip.load("ViT-L/14", device="cpu", jit=True)

import urllib
import io
import numpy as np
def download_image(url):
    urllib_request = urllib.request.Request(
        url,
        data=None,
        headers={"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0"},
    )
    with urllib.request.urlopen(urllib_request, timeout=10) as r:
        img_stream = io.BytesIO(r.read())
    return img_stream
def normalized(a, axis=-1, order=2):
    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))
    l2[l2 == 0] = 1
    return a / np.expand_dims(l2, axis)

def get_text_emb(text):
    with torch.no_grad():
        text_emb = model.encode_text(clip.tokenize([text], truncate=True).to("cpu"))
        text_emb /= text_emb.norm(dim=-1, keepdim=True)
        text_emb = text_emb.cpu().detach().numpy().astype("float32")[0]
    return text_emb

from PIL import Image as pimage

def get_image_emb(image_path):
    with torch.no_grad():
        image = pimage.open(image_path)
        image_emb = model.encode_image(preprocess(image).unsqueeze(0).to("cpu"))
        image_emb /= image_emb.norm(dim=-1, keepdim=True)
        image_emb = image_emb.cpu().detach().numpy().astype("float32")[0]
        return image_emb

def get_image_url_emb(image_url):
    succeed = True
    with torch.no_grad():
        try:
          image = pimage.open(download_image(image_url))
        except:
          print("error in reading url")
          succeed = False
          return succeed, []
        image_emb = model.encode_image(preprocess(image).unsqueeze(0).to("cpu"))
        image_emb /= image_emb.norm(dim=-1, keepdim=True)
        image_emb = image_emb.cpu().detach().numpy().astype("float32")[0]
        return succeed, image_emb

test_emb = get_image_emb("/content/test_imgs/Bryant/1581826075_bam-adebayo-skills.jpg")
test_emb_result = client.query(embedding_input=test_emb.tolist())
url = test_emb_result[0]['url']
print(url)
succeed, test_emb = get_image_url_emb(url)
print(len(test_emb_result))
print(succeed)
test_emb.shape
red_tshirt_text_emb =  get_text_emb("red tshirt")
red_tshirt_results = client.query(embedding_input=red_tshirt_text_emb.tolist())
log_result(red_tshirt_results[0])
blue_dress_image_emb = get_image_emb("https://rukminim1.flixcart.com/image/612/612/kv8fbm80/dress/b/5/n/xs-b165-royal-blue-babiva-fashion-original-imag86psku5pbx2g.jpeg?q=70")
blue_dress_results = client.query(embedding_input=blue_dress_image_emb.tolist())
log_result(blue_dress_results[0])
red_tshirt_text_emb =  get_text_emb("red tshirt")
blue_dress_image_emb = get_image_emb("https://rukminim1.flixcart.com/image/612/612/kv8fbm80/dress/b/5/n/xs-b165-royal-blue-babiva-fashion-original-imag86psku5pbx2g.jpeg?q=70")
mean_emb = normalized(red_tshirt_text_emb + blue_dress_image_emb)[0]
mean_results = client.query(embedding_input=mean_emb.tolist())
log_result(mean_results[0])
## Download and format a dataset from the results of a query

If you have some images of your own, you can query each one and collect the results into a custom dataset (a small subset of LAION-5B)
# Create urls from known images in repo
import json
from tqdm import tqdm
test_images = [f"{IMAGE_BASE_URL}{image}" for image in ["123_456.jpg", "208_495.jpg", "321_421.jpg", "389_535.jpg", "416_264.jpg", "456_123.jpg", "524_316.jpg"]]

# Re-initialize client with higher num_images
client = ClipClient(url="https://knn.laion.ai/knn-service", indice_name="laion5B-L-14", num_images=40)

# Run one query per image
combined_results = []
for image in tqdm(test_images):
    combined_results.extend(client.query(image=image))

# Save results to json file
with open("search-results.json", "w") as f:
    json.dump(combined_results, f)
!img2dataset "search-results.json" --input_format="json" --caption_col "caption" --output_folder="laion-enhanced-dataset" --resize_mode="no" --output_format="files"
## Download Kaggle Dataset
from google.colab import drive
drive.mount('/content/gdrive')
from google.colab import files
files.upload() #this will prompt you to upload the kaggle.json
!ls -lha kaggle.json
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!pwd
!kaggle datasets download -d djjerrish/nba-player-image-dataset-201920
!unzip nba-player-image-dataset-201920.zip
#@title

import os
print(f"Done! Download/copy the contents of {os.getcwd()}/laion-enhanced-dataset/")
!realpath laion-enhanced-dataset/
#@title

!wget https://openaipublic.azureedge.net/clip/data/country211.tgz
!tar zxvf country211.tgz
#@title

!pip install git+https://github.com/openai/CLIP.git
#@title

!ls country211/test/AD/1320521_42.455507_1.462726.jpg
#@title

len(test_result)
image_dict = {'kobe':[1,2,3,4],'lebron':[5,6,7,8]}
player_list = []
for each_player in player_list:
  embedding_dict = {}
  for each_img in image_dict[each_player]:
    # Find the embedding 
    player_image_emb = get_image_emb(each_img)
    embedding_dict[each_player].append(player_image_emb)

# Calculate 95% threshold on distance
# t


# data poisoning
# 1. Fawkes embedding

# 2. Lowkey embedding

# 3. original embedding


poisoning = [original, Fawkes, Lowkey]
for each embedding in image_emb:
  for poison in poisoning:
    if Fawkes or Lowkey:
      embedding = generate_poisoning(embedding)
    test_result = client.query(embedding_input=embedding.tolist())
    length = len(test_result)
    for result in test_result:
      result_emb = get_image_emb(result)
      dist = distance(result_emb, embedding)
      if dist > t:
        # remove result from test_result
      new_length = len(test(result))
      accuracy[poison] += new_length / length



import os
import shutil
rootdir = "/content"

def generate_poisoning(root):
  source_dir = "./test_imgs"
  target_dir = "./test_imgs_Fawkes"
  os.makedirs(os.path.join(rootdir, 'test_imgs_Fawkes'), exist_ok=True)
  img_list = os.listdir(source_dir)
  for img_name in img_list:
    if "_cloaked" in img_name:
      shutil.move(os.path.join(source_dir, img_name), target_dir)

poisoning = [original,fawkes,lowkey]

# def calculate_threshold(image_emb):
  
def original_search(image_emb,t):
  acc_list = []
  for embedding in image_emb:
    search_result = client.query(embedding_input = embedding.tolist())
    length = len(search_result)
    tmp_list = []
    for result in search_result:
      result_emb = get_image_emb(result)
      distance = torch.dist(result_emb,embedding) 
      if distance < t:
        tmp_list.append(result)
    acc = len(tmp_list)/length
    
    
    acc_list.append(acc)
    final_acc = sum(acc_list)/len(acc_list)

import numpy as np
a = {}
a['a'] = [1,2]

t = np.percentile((img_emb - embedding),95)
abs(img_emb-embedding).shape
import os
import tqdm
player_rootdir = '/content/test_imgs'
player_names = os.listdir(player_rootdir) 
accuracy_for_players = {}
total_acc = []
for player in player_names:
  # generate_poisonng(player)
  # Generate embedding for each image for this one player
  img_emb = []
  for player_img in os.listdir(os.path.join(player_rootdir, player)):
    print(player_img)
    img_emb.append(get_image_emb(os.path.join(player_rootdir, player, player_img)))
    accuracy_for_players[player] = []
  # Now, set one embedding as the query image and calculate the accuracy for this image
  acc_list = []
  for embedding in img_emb:
    # Calculate threshold
    # exclued_list = [x for x in img_emb if x != embedding]
    t = np.percentile(abs(img_emb-embedding),95)
    # Query image from client
    search_result = client.query(embedding_input = embedding.tolist())
    length = len(search_result)
    if length == 0:
      continue
    tmp_list = []
    for result in search_result:
      url = result['url']
      succeed, result_emb = get_image_url_emb(url)
      # result_emb = get_image_emb(result)
      if succeed:
        distance = torch.dist(torch.from_numpy(result_emb),torch.from_numpy(embedding)) 
        #if distance < t:
        tmp_list.append(result)
    acc = len(tmp_list)/length
    total_acc.append(acc)
    acc_list.append(acc)
  accuracy_for_players[player] = acc_list

total_acc = sum(total_acc)/len(total_acc)

accuracy_for_players
total_acc
test_image_emb = get_image_emb("https://storage.googleapis.com/kagglesdsdata/datasets/708260/1235942/NBA%20Players/Batum%2C%20Nicolas/513789904-e1459310098147.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20230225%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230225T060020Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=a25c8c397d6f2ceb36c49f287f25e594f60e0b4e98b6cd4a33bf3ad16b3460a27ba15c7c1da3bb3ea1187e966d07972ca274588908ba1b3dd95532dc2ee9dc62fe9c6535b0ab514bd7aff1e278b065e600bdbea88a6274a58826ae9444ff75e0d6acd0287c72c727b79f70c04f4766c801bc4c5a4c01d3b788603c4a7804967cb4915e20cd415bd01e7f259079c8cc39aca20bb82d1319d693a0d760545e65133522b04c1b83c475b160560479aa1084ce00c40b9646a1bc524f7d7a6e7c66f2758d12e78dbfabd0b298db3f3758198c2e9ff954ae637594f72d1be02cfd4b30ccfb24e45982f51ecbc0dddbd5264bd984d743a77cb07a0569c3b9a258b38bfd")
test_result = client.query(embedding_input=test_image_emb.tolist())
log_result(test_result[11])
log_result(test_result[1])
pimage.open('country211/test/AD/1320521_42.455507_1.462726.jpg')
# Fawkes
!pip3 install fawkes
!fawkes
!fawkes -d ./imgs --mode low
!python3 ./fawkes/setup.py
%cd ~
!pwd
import os
os.chdir('/content/fawkes')
print(os.getcwd())

!python3 setup.py install
!pip install numpy==1.21
'''
fawkes 1.0.4 requires keras==2.4.3, 
fawkes 1.0.4 requires tensorflow==2.4.1,
'''
!pip install tensorflow==2.4.1
!pip install keras==2.4.3
!python3 ./fawkes/fawkes/protection.py -d ./test_imgs/ --mode low
# lowkey
!unzip supp\ material.zip
!python3 ./supp\ material/attack_dir_warp.py ./test_imgs/
!pip install -r ./supp\ material/requirements.txt
!git clone https://github.com/S-aiueo32/lpips-pytorch.git
!pip install git+https://github.com/S-aiueo32/lpips-pytorch.git
!python3 ./supp\ material/lowkey_attack.py --dir ./test_imgs
!pip install torchvision==0.7.0
```