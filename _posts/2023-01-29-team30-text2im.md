---
layout: post
comments: true
title: Text Guided Image Generation
author: Alex Xia
date: 2022-01-29
---


> Creating images like illustrations, paintings, and photographs can be time-consuming and require specialized skills. A tool that can generate realistic images from natural language can make it much easier for people to create a wide range of visual content. Additionally, being able to edit images using natural language makes it possible to make precise changes quickly, which is essential in practical applications. This post is a review of different techniques that relate to image generation from text inputs.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Three Relevant Research Papers
1. ##### GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
  - [Paper] https://arxiv.org/abs/2112.10741
  - [Code] https://github.com/openai/glide-text2im
2. ##### Hierarchical Text-Conditional Image Generation with CLIP Latents
  - [Paper] https://arxiv.org/abs/2204.06125
  - [Code] https://github.com/openai/CLIP
3. ##### Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding
  - [Paper] https://arxiv.org/abs/2205.11487 
  - [Code] https://github.com/lucidrains/imagen-pytorch

## Week 7 Midterm Report

From the relevant research papers below, in this paper, I would like to investigate on text-guided diffusion models used to generate high-quality synthetic images. While there are different types of diffusion models, the type that I would like to focus on are classifier-free guidance models. 

### The Model

As such, the baseline model that I would like to implement is Open-AI's GLIDE model. The GLIDE framework comprises of three primary components. Firstly, an Ablated Diffusion Model (ADM) is utilized to create a 64 x 64 image. Secondly, a text model (transformer) influences the image generation process by using a text prompt. Lastly, an upsampling model is utilized to enlarge the image to 256 x 256 pixels, which enhances its interpretability. The ADM and text model collaborate to generate images that reflect the text prompt accurately, while the upsampling model is essential for creating more easily understandable images.

This is the code for my baseline model:

```py
from PIL import Image
from IPython.display import display
import torch as th

from glide_text2im.download import load_checkpoint
from glide_text2im.model_creation import (
    create_model_and_diffusion,
    model_and_diffusion_defaults,
    model_and_diffusion_defaults_upsampler
)

has_cuda = th.cuda.is_available()
device = th.device('cpu' if not has_cuda else 'cuda')

# Create base model.
options = model_and_diffusion_defaults()
options['use_fp16'] = has_cuda
options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling
model, diffusion = create_model_and_diffusion(**options)
model.eval()
if has_cuda:
    model.convert_to_fp16()
model.to(device)
model.load_state_dict(load_checkpoint('base', device))
print('total base parameters', sum(x.numel() for x in model.parameters()))

# Create upsampler model.
options_up = model_and_diffusion_defaults_upsampler()
options_up['use_fp16'] = has_cuda
options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling
model_up, diffusion_up = create_model_and_diffusion(**options_up)
model_up.eval()
if has_cuda:
    model_up.convert_to_fp16()
model_up.to(device)
model_up.load_state_dict(load_checkpoint('upsample', device))
print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))
def show_images(batch: th.Tensor):
    """ Display a batch of images inline. """
    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()
    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])
    display(Image.fromarray(reshaped.numpy()))


# Sampling parameters
prompt = "an oil painting of a corgi"
batch_size = 1
guidance_scale = 3.0

# Tune this parameter to control the sharpness of 256x256 images.
# A value of 1.0 is sharper, but sometimes results in grainy artifacts.
upsample_temp = 0.997


##############################
# Sample from the base model #
##############################

# Create the text tokens to feed to the model.
tokens = model.tokenizer.encode(prompt)
tokens, mask = model.tokenizer.padded_tokens_and_mask(
    tokens, options['text_ctx']
)

# Create the classifier-free guidance tokens (empty)
full_batch_size = batch_size * 2
uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(
    [], options['text_ctx']
)

# Pack the tokens together into model kwargs.
model_kwargs = dict(
    tokens=th.tensor(
        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device
    ),
    mask=th.tensor(
        [mask] * batch_size + [uncond_mask] * batch_size,
        dtype=th.bool,
        device=device,
    ),
)

# Create a classifier-free guidance sampling function
def model_fn(x_t, ts, **kwargs):
    half = x_t[: len(x_t) // 2]
    combined = th.cat([half, half], dim=0)
    model_out = model(combined, ts, **kwargs)
    eps, rest = model_out[:, :3], model_out[:, 3:]
    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)
    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)
    eps = th.cat([half_eps, half_eps], dim=0)
    return th.cat([eps, rest], dim=1)

# Sample from the base model.
model.del_cache()
samples = diffusion.p_sample_loop(
    model_fn,
    (full_batch_size, 3, options["image_size"], options["image_size"]),
    device=device,
    clip_denoised=True,
    progress=True,
    model_kwargs=model_kwargs,
    cond_fn=None,
)[:batch_size]
model.del_cache()

# Show the output
show_images(samples)


##############################
# Upsample the 64x64 samples #
##############################

tokens = model_up.tokenizer.encode(prompt)
tokens, mask = model_up.tokenizer.padded_tokens_and_mask(
    tokens, options_up['text_ctx']
)

# Create the model conditioning dict.
model_kwargs = dict(
    # Low-res image to upsample.
    low_res=((samples+1)*127.5).round()/127.5 - 1,

    # Text tokens
    tokens=th.tensor(
        [tokens] * batch_size, device=device
    ),
    mask=th.tensor(
        [mask] * batch_size,
        dtype=th.bool,
        device=device,
    ),
)

# Sample from the base model.
model_up.del_cache()
up_shape = (batch_size, 3, options_up["image_size"], options_up["image_size"])
up_samples = diffusion_up.ddim_sample_loop(
    model_up,
    up_shape,
    noise=th.randn(up_shape, device=device) * upsample_temp,
    device=device,
    clip_denoised=True,
    progress=True,
    model_kwargs=model_kwargs,
    cond_fn=None,
)[:batch_size]
model_up.del_cache()

# Show the output
show_images(up_samples)
```

### Experiments

I plan on doing an investigative research on how this GLIDE model works and how different combinations of different hyperparameters can lead to a better understanding of the model as well as better performance. I think this is important because I have noticed that while GLIDE is able to produce very complex and high-quality images, it still has a lot of limitations. For example, one weakness is that the scope of what kind of images it can generate is pretty small. It also has a very high computational cost. Perhaps we can tune some of the hyperparameters such as noise level for the diffusion, to see what kind of changes it can bring about.

## Reference
Please make sure to cite properly in your work, for example:

[1] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., & Chen, M. (2021). GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. ArXiv. https://doi.org/10.48550/arXiv.2112.10741

[2] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents. ArXiv. https://doi.org/10.48550/arXiv.2204.06125

[3] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., & Norouzi, M. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. ArXiv. https://doi.org/10.48550/arXiv.2205.11487

---
